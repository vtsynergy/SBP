import argparse
import os
# from partition_baseline_support import *
import graph_tool.all as gt
import pandas as pd # for writing output graph TSV files
import numpy as np
import random
import scipy.stats as stats

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-n", "--numvertices", type=int, default=200, help="Number of vertices in graph")
    parser.add_argument("-c", "--communities", type=int, default=-1, help="Number of communities in graph. If -1, uses communityexponent argument instead.")
    parser.add_argument("-m", "--communityexponent", type=float, default=0.35, help="Number of communities = n^m. Only used if communities is -1.")
    # parser.add_argument("-i", "--mindegree", type=int, default=10, help="Min vertex degree = min(i, n / 4c)")
    parser.add_argument("-a", "--maxdegree", type=float, default=0.05, help="Max vertex degree = a * n")
    parser.add_argument("-o", "--overlap", type=float, default=5.0, help="5 = low, 2 (or 1.7) = high")
    parser.add_argument("-s", "--blocksizevariation", type=float, default=1.0, help="1 = low, 5 = high")
    parser.add_argument("-e", "--powerlawexponent", type=float, default=-2.1, help="The power law exponent")
    parser.add_argument("-d", "--density", type=float, default=1.0, help="1 - d fraction of edges will be removed")
    parser.add_argument("--directory", type=str, default="/mnt/d/workspace/graph_data")
    return parser.parse_args()
# Generate the graph according to the blockmodel and parameters

args = parse_args()
N = args.numvertices
N_adjusted = int(args.numvertices * 1.13)
C = args.communities
M = args.communityexponent
min_degree = 1  # args.mindegree
max_degree = int(args.maxdegree * N_adjusted)  # A = args.maxdegree
ratio_within_over_between = args.overlap
block_size_heterogeneity = args.blocksizevariation
powerlaw_exponent = args.powerlawexponent
density = args.density
num_blocks = C
if num_blocks == -1:
    num_blocks = int(N_adjusted ** M)  # number of blocks grows sub-linearly with number of nodes. Exponent is a parameter.
print('Number of blocks: {}'.format(num_blocks))

# N_adjusted = 200  # number of nodes
save_graph = True
tag = "test_{0}_{1}_{2}_{3}".format(num_blocks, args.maxdegree, powerlaw_exponent, density)
overlap = "low"
if args.overlap < 5:
    overlap = "high"
block_size_variation = "low"
if args.blocksizevariation > 1.0:
    block_size_variation = "high"
file_name = "{4}/{3}/{0}Overlap_{1}BlockSizeVar/{3}_{0}Overlap_{1}BlockSizeVar_{2}_nodes".format(
    args.overlap, args.blocksizevariation, N, tag, args.directory)
os.makedirs(os.path.dirname(file_name), exist_ok=True)
# file_name = './simulated_blockmodel_graph_{:d}_nodes'.format(N)  # output file name

# parameters for the Power-Law degree distribution
# powerlaw_exponent = -2.5
# max_degree = A
# min_degree = min(I, int(N / (num_blocks * 4)))  # node degree range is adjusted lower when the blocks have few nodes
# max_degree = min(A, int(N / num_blocks))

# if min_degree < 1:
#     min_degree = 1
# if max_degree > N:
#     max_degree = N

# sparsity parameter (1-density fraction of the edges will be removed)
# density = 1


# define discrete power law distribution
def discrete_power_law(a, min_v, max_v):
    x = np.arange(min_v, max_v + 1, dtype='float')
    pmf = x ** a
    pmf /= pmf.sum()
    return stats.rv_discrete(values=(x, pmf))

print("expected degrees: [{},{}]".format(min_degree, max_degree))
# set in-degree and out-degree distribution
rv_indegree = discrete_power_law(powerlaw_exponent, min_degree, max_degree)
rv_outdegree = discrete_power_law(powerlaw_exponent, min_degree, max_degree)

# define the return function for in and out degrees
def degree_distribution_function(rv1, rv2):
    return (rv1.rvs(size=1), rv2.rvs(size=1))

# this parameter adjusts the ratio between the total number of within-block edges and between-block edges
# ratio_within_over_between = 5


# set the within-block and between-block edge strength accordingly
def inter_block_strength(a, b):
    if a == b:  # within block interaction strength
        return 1
    else:  # between block interaction strength
        avg_within_block_nodes = float(N_adjusted) / num_blocks
        avg_between_block_nodes = N_adjusted - avg_within_block_nodes
        return avg_within_block_nodes / avg_between_block_nodes / ratio_within_over_between


# draw block membership distribution from a Dirichlet random variable
# block_size_heterogeneity = 1  # 3; # larger means the block sizes are more uneven
block_distribution = np.random.dirichlet(np.ones(num_blocks) * 10 / block_size_heterogeneity, 1)[0]
# print('Block distribution: {}'.format(block_distribution))

# draw block membership for each node
block_membership_vector = np.where(np.random.multinomial(n=1, size=N_adjusted, pvals=block_distribution))[1]
# renumber this in case some blocks don't have any elements
blocks, counts = np.unique(block_membership_vector, return_counts = True)
block_mapping = {value:index for index, value in enumerate(blocks)}
block_membership_vector = np.asarray([
    block_mapping[block_membership_vector[i]] for i in
    range(block_membership_vector.size)
])
num_blocks = blocks.size

####################
# GENERATE DEGREE-CORRECTED SBM
####################
blocks, counts = np.unique(block_membership_vector, return_counts=True)
block_edge_propensities = np.zeros((num_blocks, num_blocks), dtype=np.float32)
for row in range(num_blocks):
    for col in range(num_blocks):
        strength = inter_block_strength(row, col)
        value = strength * counts[row] * counts[col]
        # block_edge_propensities[row,col] = inter_block_strength(row, col)
        block_edge_propensities[row,col] = value
out_degrees = rv_outdegree.rvs(size=N_adjusted)
in_degrees = rv_indegree.rvs(size=N_adjusted)

total_degrees = rv_outdegree.rvs(size=N_adjusted)
out_degrees = np.random.uniform(size=N_adjusted) * total_degrees
out_degrees = np.round(out_degrees)
in_degrees = total_degrees - out_degrees
sum_degrees = total_degrees.sum()
print("sum degrees: ", sum_degrees)

# Let's say we want E = 7V
# expected_e = 7 * N
expected_e = sum_degrees
K = expected_e / (np.sum(out_degrees + in_degrees))
# out_degrees = K * out_degrees
# in_degrees = K * in_degrees

print("out: [{},{}]".format(np.min(out_degrees), np.max(out_degrees)))
print("in: [{},{}]".format(np.min(in_degrees), np.max(in_degrees)))
# print("B:\n", block_edge_propensities)
g_sample = gt.generate_sbm(
        # Block membership of each vertex
        b=block_membership_vector,
        # Edge propensities between communities
        probs=block_edge_propensities * (expected_e /
                                         block_edge_propensities.sum()),
        # The out degree propensity of each vertex
        out_degs=out_degrees,
        # The in degree propensity of each vertex
        in_degs=in_degrees,
        directed=True,
        micro_ers=False,  # If True, num edges b/n groups will be exactly probs
        micro_degs=False  # If True, degrees of nodes will be exactly degs
)
# generate the graph
# if (float(gt.__version__[0:4]) >= 2.20):  # specify inter-block strength through edge_probs in later versions
#     g_sample, block_membership = gt.random_graph(N, lambda: degree_distribution_function(rv_indegree, rv_outdegree), \
#                                                  directed=True, model="blockmodel",
#                                                  block_membership=block_membership_vector,
#                                                  edge_probs=inter_block_strength,
#                                                  n_iter=50, verbose=False)
# else:  # specify inter-block strength through vertex_corr in earlier versions
#     g_sample, block_membership = gt.random_graph(N, lambda: degree_distribution_function(rv_indegree, rv_outdegree), \
#                                                  directed=True, model="blockmodel",
#                                                  block_membership=block_membership_vector,
#                                                  vertex_corr=inter_block_strength,
#                                                  n_iter=50, verbose=False)

# remove (1-density) percent of the edges
edge_filter = g_sample.new_edge_property('bool')
edge_filter.a = stats.bernoulli.rvs(density, size=edge_filter.a.shape)
# print("edge filter: ", edge_filter.a)
g_sample.set_edge_filter(edge_filter)
g_sample.purge_edges()

# remove all island vertices
print('Filtering out zero vertices...')
degrees = g_sample.get_total_degrees(np.arange(g_sample.num_vertices()))
vertex_filter = g_sample.new_vertex_property('bool', vals=degrees > 0.0)
g_sample.set_vertex_filter(vertex_filter)
g_sample.purge_vertices()

# store the nodal block memberships in a vertex property
block_membership_vector = block_membership_vector[degrees > 0.0]
true_partition = block_membership_vector
assert block_membership_vector.size == g_sample.num_vertices()
block_membership = g_sample.new_vertex_property("int",
                                                vals=block_membership_vector)

# compute and report basic statistics on the generated graph
bg, bb, vcount, ecount, avp, aep = gt.condensation_graph(g_sample, block_membership, self_loops=True)
edge_count_between_blocks = np.zeros((num_blocks, num_blocks))
for e in bg.edges():
    edge_count_between_blocks[bg.vertex_index[e.source()], bg.vertex_index[e.target()]] = ecount[e]
num_within_block_edges = sum(edge_count_between_blocks.diagonal())
num_between_block_edges = g_sample.num_edges() - num_within_block_edges
# print count statistics
print('Number of nodes: {} expected {} filtered % {}'.format(
    g_sample.num_vertices(), N, (N_adjusted - g_sample.num_vertices()) / N_adjusted))
print('Number of edges: {} expected number of edges: {}'.format(
    g_sample.num_edges(), expected_e))
degrees = g_sample.get_total_degrees(np.arange(g_sample.num_vertices()))
print('Vertex degrees: [{},{},{}]'.format(
    np.min(degrees), np.mean(degrees), np.max(degrees)))
unique_degrees, counts = np.unique(degrees, return_counts=True)
print("degrees: {}\ncounts: {}".format(unique_degrees[:20], counts[:20]))
print('Avg. Number of nodes per block: {}'.format(g_sample.num_vertices() / num_blocks))
print('# Within-block edges / # Between-blocks edges: {}'.format(num_within_block_edges / num_between_block_edges))

if save_graph:  # output the graph and truth partition to TSV files with standard format
    g_sample.save('{}.gt.bz2'.format(file_name))  # save graph-tool graph object
    # store edge list
    edge_list = np.zeros((g_sample.num_edges(), 3), dtype=int)
    # populate the edge list.
    counter = 0
    for e in g_sample.edges():  # iterate through all edges (edge list access is not available in all versions of graph-tool)
        edge_list[counter, 0] = int(e.source()) + 1;  # nodes are indexed starting at 1 in the standard format
        edge_list[counter, 1] = int(e.target()) + 1;  # nodes are indexed starting at 1 in the standard format
        edge_list[counter, 2] = 1;  # all edges are weighted equally at 1 in this generator
        counter += 1
    # write graph TSV file using pandas DataFrame
    df_graph = pd.DataFrame(edge_list)
    df_graph.to_csv('{}.tsv'.format(file_name), sep='\t', header=False, index=False)
    # write truth partition TSV file using pandas DataFrame; nodes and blocks are indexed starting at 1 in the standard format
    df_partition = pd.DataFrame(np.column_stack((np.arange(g_sample.num_vertices()) + 1, true_partition + 1)))
    df_partition.to_csv('{}_truePartition.tsv'.format(file_name), sep='\t', header=False, index=False)

